{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c13cdf",
   "metadata": {},
   "source": [
    "## Agent \n",
    "Quesqu'un **agent IA**\n",
    "\n",
    "  Les **agents d'IA** sont des systèmes logiciels qui utilisent l'IA pour atteindre des objectifs et effectuer des tâches au nom des utilisateurs. Ils font preuve de raisonnement, de planification et de mémoire, et disposent d'un certain niveau d'autonomie pour prendre des décisions, apprendre et s'adapter.\n",
    "\n",
    "  Leurs capacités sont en grande partie rendues possibles par la **capacité multimodale** de **l'IA générative** et des modèles de fondation d'IA. Les agents d'IA peuvent traiter simultanément des informations multimodales telles que du texte, de la voix, des vidéos, des sons, du code, etc. Ils peuvent converser, raisonner, apprendre et prendre des décisions. Ils peuvent apprendre au fil du temps et faciliter les transactions et les processus métier. Les agents peuvent collaborer avec d'autres agents pour coordonner et exécuter des workflows plus complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain import hub\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pprint import pprint\n",
    "\n",
    "# Chargement des clés d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des modèles en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "#model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda05d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation des outils\n",
    "def data_right_disability(query):\n",
    "    message =\"Voici les droits sur les handicapées\"\n",
    "    return message\n",
    "\n",
    "def data_right_child(query):\n",
    "    message =\"Voici les droits sur les enfants\"\n",
    "    return message\n",
    "\n",
    "def data_right_accommodation(query):\n",
    "    message =\"Voici les droits sur les logements\"\n",
    "    return message\n",
    "\n",
    "def error_handle_message(query):\n",
    "    message =f\"Je n'ai pas compris la requête veuillez reformuler votre message '{query}'\"\n",
    "    return \"Je n'ai pas compris la requête veuillez reformuler votre message\"\n",
    "\n",
    "tools=[\n",
    "    Tool(\n",
    "        name=\"right_disability\",\n",
    "        func=data_right_disability,\n",
    "        description=\"\"\"\n",
    "        Use this tool if the query is asking right,help,service about disability stay polite and helpful.\n",
    "        If it's french right use the data documentary if possible if not use internet.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"right_child\",\n",
    "        func=data_right_child,\n",
    "        description=\"\"\"\n",
    "        Use this tool if the query is asking right,help,service about children stay polite and helpful.\n",
    "        If it's french right use the data documentary if possible if not use internet.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"right_accommodation\",\n",
    "        func=data_right_accommodation,\n",
    "        description=\"\"\"\n",
    "        Use this tool if the query is asking right,help,service about accomodation stay polite and helpful.\n",
    "        If it's french right use the data documentary if possible if not use internet.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"generate_message\",\n",
    "        func=error_handle_message,\n",
    "        description=\"\"\"\n",
    "        Use this tool if the query is not right about disability, child or accommodation and quit ask to rewrite\n",
    "        the question. If it's unclearn but it's to much unclear query quit the conversation and stay polite\"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844d7fc",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac273df",
   "metadata": {},
   "source": [
    "le choix ce porte pour **rlm/rag-prompt** car il est plus adapter pour le chat les question/réponse.\n",
    "\n",
    "\n",
    "  source : https://smith.langchain.com/hub/rlm/rag-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du prompt standard pour le paradigme ReAct depuis LangChain Hub\n",
    "#prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# Initialisation de la mémoire pour suivre l’historique des échanges\n",
    "memory= ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#creation agent\n",
    "agent=create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    stop_sequence=True\n",
    "    \n",
    ")\n",
    "\n",
    "#encapsulation agent\n",
    "#enlever dans la version final mettre Verbose a false\n",
    "executor=AgentExecutor.from_agent_and_tools( \n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    max_iterations=5,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle interactive terminale\n",
    "while True:\n",
    "    user_input = input(\"Vous : \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage précédent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requête de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    response = executor.invoke({\"input\": user_input})\n",
    "    display(Markdown(response[\"output\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
