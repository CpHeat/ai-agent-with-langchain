{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c13cdf",
   "metadata": {},
   "source": [
    "## Agent \n",
    "Quesqu'un **agent IA**\n",
    "\n",
    "  Les **agents d'IA** sont des systèmes logiciels qui utilisent l'IA pour atteindre des objectifs et effectuer des tâches au nom des utilisateurs. Ils font preuve de raisonnement, de planification et de mémoire, et disposent d'un certain niveau d'autonomie pour prendre des décisions, apprendre et s'adapter.\n",
    "\n",
    "  Leurs capacités sont en grande partie rendues possibles par la **capacité multimodale** de **l'IA générative** et des modèles de fondation d'IA. Les agents d'IA peuvent traiter simultanément des informations multimodales telles que du texte, de la voix, des vidéos, des sons, du code, etc. Ils peuvent converser, raisonner, apprendre et prendre des décisions. Ils peuvent apprendre au fil du temps et faciliter les transactions et les processus métier. Les agents peuvent collaborer avec d'autres agents pour coordonner et exécuter des workflows plus complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6115125e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, clear_output, Markdown\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain import hub\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pprint import pprint\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from settings import vectorizing_params, retriever_params\n",
    "# Chargement des clés d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des modèles en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "#model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f1f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 0 chunks.\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"data\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=vectorizing_params['chunk_size'],\n",
    "    chunk_overlap=vectorizing_params['chunk_overlap']\n",
    ")\n",
    "\n",
    "documents = []\n",
    "\n",
    "if not os.path.exists(db_dir):\n",
    "    print(\"Initializing vector store...\")   \n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    full_text = f.read()\n",
    "\n",
    "                # Extract themes and subthemes from the file structure\n",
    "                relative_path = os.path.relpath(file_path, \"data\")\n",
    "                parts = relative_path.split(os.sep)\n",
    "                large_theme = parts[0]\n",
    "                theme = parts[1]\n",
    "                subtheme = parts[2].replace(\".txt\", \"\")\n",
    "\n",
    "                # Chunk splitting\n",
    "                chunks = text_splitter.split_text(full_text)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    documents.append(\n",
    "                        Document(\n",
    "                            page_content=chunk,\n",
    "                            metadata={\n",
    "                                \"large_theme\": large_theme,\n",
    "                                \"theme\": theme,\n",
    "                                \"subtheme\": subtheme,\n",
    "                                \"chunk_id\": i,\n",
    "                                \"source\": file_path\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents,\n",
    "        embedding=embedder,\n",
    "        collection_name=\"droits\",\n",
    "        persist_directory=db_dir\n",
    "    )\n",
    "else:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=db_dir,\n",
    "        embedding_function=embedder,\n",
    "        collection_name=\"droits\"\n",
    "    )\n",
    "\n",
    "#db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)\n",
    "\n",
    "print(f\"Vectorstore created with {len(documents)} chunks.\")\n",
    "\n",
    "#db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=retriever_params['search_type'],\n",
    "    search_kwargs=retriever_params['search_kwargs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda05d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever)\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide à trouver des informations concernant les droits disponibles.\")\n",
    "]\n",
    "def _format_docs(docs):\n",
    "    \n",
    "    result = \"\\n\".join(\n",
    "        [\n",
    "            f'<item index=\"{i+1}\">\\n<page_content>\\n{r}\\n</page_content>\\n</item>'\n",
    "            for i, r in enumerate(docs)\n",
    "        ]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def format_agent_scratchpad(intermediate_steps):\n",
    "    thoughts = \"\"\n",
    "    for action, observation in intermediate_steps:\n",
    "        thoughts += action.log\n",
    "        thoughts += \"</search_query>\" + _format_docs(observation)\n",
    "    return thoughts\n",
    "\n",
    "def ask_rag(query: str) -> str:\n",
    "\n",
    "    relevant_chunks = retriever.invoke(query)\n",
    "            \n",
    "    input_message = (\n",
    "        \"Voici des documents qui vont t'aider à répondre à la question : \"\n",
    "        + query\n",
    "        + \"\\n\\nDocuments pertinents : \\n\"\n",
    "        + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "        + \"\\n\\nDonne une réponse basée uniquement sur les documents qui te sont fournis.\"\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\n",
    "        \"question\": input_message,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    chat_history.append((query, result[\"answer\"]))\n",
    "    return result[\"answer\"]\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"consult_droit\",\n",
    "    func=ask_rag,\n",
    "    description=\"Répond à des questions sur les droits sociaux (APL, RSA, etc.). Fournit des réponses fiables extraites de documents organisés par thème.\"\n",
    ")\n",
    "tools=[\n",
    "    rag_tool\n",
    "]\n",
    "\n",
    "tool_names=\"consult_droit\"\n",
    "\n",
    "agent_scratchpad=chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844d7fc",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac273df",
   "metadata": {},
   "source": [
    "le choix ce porte pour **rlm/rag-prompt** car il est plus adapter pour le chat les question/réponse.\n",
    "\n",
    "\n",
    "  source : https://smith.langchain.com/hub/rlm/rag-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a87ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Role: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.If your source is from internet use italic and bold font.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 5 times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b315fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du prompt standard pour le paradigme ReAct depuis LangChain Hub\n",
    "#prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "# Initialisation de la mémoire pour suivre l’historique des échanges\n",
    "memory= ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#creation agent\n",
    "agent=create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    stop_sequence=True\n",
    "    \n",
    ")\n",
    "\n",
    "#encapsulation agent\n",
    "#enlever dans la version final mettre Verbose a false\n",
    "executor=AgentExecutor.from_agent_and_tools( \n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    max_iterations=5,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f20e55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Boucle interactive terminale\n",
    "while True:\n",
    "    user_input = input(\"Vous : \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage précédent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requête de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    response = executor.invoke({\"input\": user_input})\n",
    "    display(Markdown(response[\"output\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
